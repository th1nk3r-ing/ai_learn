# <font color=#0099ff> **深度学习 - 3B1B** </font>

> `@think3r` 2025-02-17 23:05:14
>
>1. [**3B1B --> 深度学习之神经网络的结构**](https://space.bilibili.com/88461692/channel/seriesdetail?sid=1528929)
>    - [【中文配音】深度学习（3Blue1Brown）](https://www.bilibili.com/video/BV1szpFedEV2)
>2. [Neural Networks](https://www.3blue1brown.com/topics/neural-networks)

手写识别系统 : 神经网络中的 "hello world"

- 第二层识别短边
- 第三层识别图案
- 第四层组合图案并得到结果
- NOTE: 以上的全是猜测.... 实际并不是这么玩的

## <font color=#009A000> 0x00 神经网络 </font>

- 激活函数
  - 线性整流函数 Rectified linear unit : $ReLU(a) = max(0, a)$
    - 更方便于训练
  - ~~Sigmoid 函数~~ :  $\sigma(x) = \frac{1}{1 + e^{-x}}$
    - 偏置值 $b0$ <---- 过早激发出了偏差
    - 过去使用较多, 为了部分模仿生物学上的神经元什么时候激发, 但它很难训练, 也并没有让训练结果变得更好

    ```py
    " https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py "
    """ b 为偏置, w 为权重(矩阵) """
    def feedforward(self, a):
        """Return the output of the network if ``a`` is input."""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a)+b)
        return a
    ```

## <font color=#009A000> 0x01 梯度下降法 Gradient descent </font>

- cost function 代价函数
- 梯度下降算法 : 一种让你收敛代价函数到最小

## <font color=#009A000> 0x02 反向传播算法 </font>
