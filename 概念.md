# <font color=#0099ff> **速通概念 --> llm 文本大模型** </font>

> `@think3r` 2025-02-08 23:50:03
>
> - [安德烈·卡帕西最新AI普及课：深入探索像ChatGPT这样的大语言模型 Andrej Karpathy](https://www.bilibili.com/video/BV1WnNHeqEFK)
>   - ori : <https://www.youtube.com/watch?v=7xTGNNLPyMI>
> - [🍷 FineWeb: decanting the web for the finest text data at scale](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)

## <font color=#009A000> 0x00 简介 </font>

<https://drive.google.com/file/d/1EZh5hNDzxMMy05uLhVryk061QYQGTxiN/view>

![llvmIntro](./image/LLMIntro-2024-07-22-1743.svg)

## <font color=#009A000> 0x01 </font>

1. 预训练 pre-trainning
   - [huggingFace-FineWeb 数据集](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) --> 过滤后的最终文本数据 `44TB`
     - CommonCrawl 的网络爬虫
     - url-filtering : <https://dsi.ut-capitole.fr/blacklists/>
     - text-extraction : 去除 html 标记
     - lang-filtering : 保留 85% 以上英文的网页
     - ... 重复信息删除
     - PII remove : personally identifiable information remove 个人信息移除
     - 最终处理结果 : <https://huggingface.co/datasets/HuggingFaceFW/fineweb?row=0> -> Dataset Preview 部分
2. tokenization :
   - Converts text <---> sequences of symbols (/tokens) 尽可能的压缩文本数据量
     - why ?
   - <https://tiktokenizer.vercel.app/>
3. neural network training :
   1. 窗口上下文数量 maximum-context-length
   2. LLM Visualization : <https://bbycroft.net/llm>
   3. gpt :
      - 论文 : attention is all you need
      - TODO: 论文阅读工具...
      - GPT2 :
        - <https://github.com/openai/gpt-2.git>
        - 论文 : <https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>
          - [论文阅读：Language Models are Unsupervised Multitask Learners](https://zhuanlan.zhihu.com/p/711058430)
        - 复现项目 : `llm.c` --> <https://github.com/karpathy/llm.c/discussions/677>
      - llama-3 :
        - 论文 : <https://arxiv.org/pdf/2407.21783>
   4. 网络结构 : `model.py`
   5. base-model 基础模型
